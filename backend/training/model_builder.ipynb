{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in stroke data from a csv \n",
    "cousin_directory_path = os.path.join(os.getcwd(), '..', 'training_data')\n",
    "cousin_directory_path = os.path.abspath(cousin_directory_path)\n",
    "\n",
    "stroke_data_filename = 'normalized_strokes.csv'\n",
    "csv_file_path = os.path.join(cousin_directory_path, stroke_data_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has string representations for each label. We create a mapping between each unique string to a unique integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = set()\n",
    "\n",
    "with open(csv_file_path, mode='r') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    next(csv_reader)  # Skip header\n",
    "    for row in csv_reader:\n",
    "        label = row[1]\n",
    "        unique_labels.add(label)\n",
    "\n",
    "sorted_labels = sorted(list(unique_labels))\n",
    "\n",
    "label_to_index = {label: index for index, label in enumerate(sorted_labels)}\n",
    "index_to_label = {index: label for label, index in label_to_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset for stroke data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrokeDataset(Dataset):\n",
    "    def __init__(self, csv_file, label_mapping):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to the csv file with stroke data.\n",
    "            label_mapping (dict): Mapping from string labels to integer indices.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.label_mapping = label_mapping\n",
    "\n",
    "        with open(csv_file, mode='r') as file:\n",
    "            csv_reader = csv.reader(file)\n",
    "            next(csv_reader)  # Skip header\n",
    "            for row in csv_reader:\n",
    "                label_str = row[1]\n",
    "                stroke_points = row[2]\n",
    "\n",
    "                # Convert stroke points from string to float tuples\n",
    "                stroke_points = json.loads(stroke_points)\n",
    "                self.data.append(stroke_points)\n",
    "                self.labels.append(self.label_mapping[label_str])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        stroke = np.array(self.data[index], dtype=np.float32)\n",
    "        label = self.labels[index]\n",
    "                \n",
    "        return stroke, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubsetStrokeDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.labels = labels \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        stroke = np.array(self.data[index], dtype=np.float32)\n",
    "        label = self.labels[index]\n",
    "\n",
    "        return stroke, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = StrokeDataset(csv_file_path, label_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes with fewer than 2 samples in temp_labels: [729, 237, 330, 845, 208, 170, 955, 286, 219, 1076, 829, 337, 154, 55, 910, 341, 673, 892, 963, 789, 805, 972, 117, 1082, 152, 272, 859, 216, 971, 228, 383, 848, 98]\n",
      "Moved 33 samples to the training set to ensure all classes in temp_labels have at least 2 samples.\n",
      "Final dataset sizes:\n",
      " - Training: 147350 samples\n",
      " - Validation: 31552 samples\n",
      " - Testing: 31552 samples\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.7\n",
    "val_size = 0.15\n",
    "test_size = 0.15\n",
    "\n",
    "train_data, temp_data, train_labels, temp_labels = train_test_split(\n",
    "    dataset.data, dataset.labels,\n",
    "    train_size=train_size,\n",
    "    random_state=42,\n",
    "    stratify=dataset.labels\n",
    ")\n",
    "\n",
    "counter_temp = Counter(temp_labels)\n",
    "insufficient_classes = [cls for cls, count in counter_temp.items() if count < 2]\n",
    "print(\"Classes with fewer than 2 samples in temp_labels:\", insufficient_classes)\n",
    "\n",
    "if insufficient_classes:\n",
    "    indices_to_move = [i for i, label in enumerate(temp_labels) if label in insufficient_classes]\n",
    "    \n",
    "    # Reassign these samples to the training set\n",
    "    for idx in sorted(indices_to_move, reverse=True):\n",
    "        train_data.append(temp_data.pop(idx))\n",
    "        train_labels.append(temp_labels.pop(idx))\n",
    "    \n",
    "    print(f\"Moved {len(indices_to_move)} samples to the training set to ensure all classes in temp_labels have at least 2 samples.\")\n",
    "\n",
    "val_data, test_data, val_labels, test_labels = train_test_split(\n",
    "    temp_data, temp_labels,\n",
    "    train_size=val_size / (val_size + test_size),\n",
    "    random_state=42,\n",
    "    stratify=temp_labels\n",
    ")\n",
    "\n",
    "train_dataset = SubsetStrokeDataset(train_data, train_labels)\n",
    "val_dataset = SubsetStrokeDataset(val_data, val_labels)\n",
    "test_dataset = SubsetStrokeDataset(test_data, test_labels)\n",
    "\n",
    "print(f\"Final dataset sizes:\\n\"\n",
    "      f\" - Training: {len(train_dataset)} samples\\n\"\n",
    "      f\" - Validation: {len(val_dataset)} samples\\n\"\n",
    "      f\" - Testing: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Handles variable length sequences in stroke data by padding them with zeros.\n",
    "\n",
    "    Args:\n",
    "        batch (List[Tuple]): Each tuple contains (stroke, label).\n",
    "\n",
    "    Returns:\n",
    "        padded_strokes (torch.Tensor): Tensor of shape (batch_size, max_seq_length, 2).\n",
    "        labels (torch.Tensor): Tensor of shape (batch_size,).\n",
    "    \"\"\"\n",
    "    strokes, labels = zip(*batch)\n",
    "    strokes = [torch.tensor(stroke, dtype=torch.float32) for stroke in strokes]\n",
    "\n",
    "    # Pad sequences with zeros to amke sure each stroke is the same length\n",
    "    padded_strokes = pad_sequence(strokes, batch_first=True, padding_value=0.0)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return padded_strokes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
